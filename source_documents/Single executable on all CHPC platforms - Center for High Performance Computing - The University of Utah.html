<!DOCTYPE HTML><html lang="en-US" class="no-js">
   <head><!-- PAGE HEAD -->
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
      <title>Single executable on all CHPC platforms - Center for High Performance Computing - The University of Utah</title>
      <meta name="keywords" content="">
      <meta name="description" content="">
      <meta name="robots" content="index,follow">
      <link rel="icon" href="//templates.utah.edu/_main-v3-1/images/template/favicon.ico">
      <link rel="apple-touch-icon-precomposed" href="//templates.utah.edu/_main-v3-1/images/template/apple-touch-icon.png">
      <link rel="stylesheet" href="//templates.utah.edu/_main-v3-1/css/main.min.css" type="text/css"><noscript>
         <link rel="stylesheet" href="//templates.utah.edu/_main-v3-1/css/assets/fontawesome/css/all.min.css" type="text/css"></noscript><link href="/_resources/css/custom.css" rel="stylesheet" type="text/css">
      <script src="//templates.utah.edu/_main-v3-1/js/head-code.min.js"></script>
      <!-- HEAD CODE -->
      
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Y160DVJ0DZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-Y160DVJ0DZ');
</script>

      
      <!-- END HEAD CODE -->
      <!-- END PAGE HEAD -->
      </head>
   <body class="has-headernav"><!-- PAGE BODY -->
      <a class="uu-skip-link" href="#skip-link-target">Skip to content</a>
      <!-- BODY TOP CODE -->
            <!-- END BODY TOP CODE -->
      
      <div id="uu-top-target" class="uu-site"><!-- SEARCH -->
         <div class="uu-search" role="search">
    <div class="uu-search__container">
        <!-- SITE SEARCH -->
        <form method="get" id="search-site" class="uu-search__form" action="/search/index.php">

            <label for="inputSearchSite" class="sr-only">Search Site:</label>
            <input type="text" id="inputSearchSite" name="q" value="" placeholder="Search Site" />
            <input type="hidden" name="gcse_action" value="site" />

            <div class="form-powered-by">
                <span>Powered by</span> 
                <img src="https://templates.utah.edu/_main-v3-1/images/template/google-logo.png" alt="Google Search">
            </div>

        </form>
        <!-- END SITE SEARCH -->
        <!-- CAMPUS SEARCH -->
        <form method="get" id="search-campus" class="uu-search__form" action="/search/index.php">

            <label for="inputSearchCampus" class="sr-only">Search Campus:</label>
            <input type="text" id="inputSearchCampus" name="q" value="" placeholder="Search Campus" />
            <input type="hidden" name="gcse_action" value="campus" />

            <div class="form-powered-by">
                <span>Powered by</span> 
                <img src="https://templates.utah.edu/_main-v3-1/images/template/google-logo.png" alt="Google Search">
            </div>
        </form>
        <!-- END CAMPUS SEARCH -->

        <!-- SEARCH TYPE TOGGLE -->
        <div class="search-type-toggle">
            <label class="uu-switch" for="search_campus_checkbox">
                <input type="checkbox" name="search_campus_checkbox" value="" id="search_campus_checkbox">
                <span class="uu-switch-slider"></span>
                <span class="uu-switch-label">Search Campus</span>
            </label>
        </div>
        <!-- END SEARCH TYPE TOGGLE -->

    </div>
</div><!-- END SEARCH -->
         <!-- HEADER -->
         
         <header class="uu-header">
            <div class="uu-header__container">
               <!-- ALERT AREA -->
               <div id="alert_bar" class="uu-alert-bar"> 
	<a href="https://coronavirus.utah.edu/">University of Utah COVID-19 Updates</a>
</div><!-- END ALERT AREA -->
               
               <div class="uu-header__top"> <a href="https://www.utah.edu/" class="uu-header__logo"><span class="sr-only">The University of Utah</span></a>                  <div class="uu-header__middle">
                     <!-- HEADER TITLE -->
                     <div class="uu-header__title">
<h2><a href="/">CHPC - Research Computing and Data Support for the University</a></h2>
<!-- <h3><a href="http://it.utah.edu">University Information Technology</a></h3> --></div><!-- END HEADER TITLE -->
                     <!-- HEADER NAVIGATION -->
                     
<nav class="uu-header__nav">
	
<ul class="uu-menu__level1">
<ul class="uu-menu__level1">
<li class="has-sub"><a href="#">About Us</a>
<ul class="sub-menu">
<li><a href="/about/index.php">About Us</a></li>
<li><a href="https://www.chpc.utah.edu/about/vision.php">Vision</a></li>
<li><a href="/about/staff.php">Staff</a></li>
<li><a href="https://www.chpc.utah.edu/about/contact.php">Contact Information</a></li>
<li><a href="https://www.chpc.utah.edu/about/acknowledge.php">Acknowledging CHPC</a></li>
<li><a href="https://www.chpc.utah.edu/highlights.php">Research Highlights</a></li>
<li><a href="https://www.chpc.utah.edu/about/partners.php">Partners</a></li>
<li><a href="/news/index.php">News</a></li>
<li><a href="/about/governance.php">Governance</a></li>
</ul>
</li>
<li class="has-sub"><a href="#">Resources</a>
<ul class="sub-menu">
<li><a href="https://www.chpc.utah.edu/resources/index.php">Resources</a></li>
<li><a href="https://www.chpc.utah.edu/resources/HPC_Clusters.php">HPC Clusters</a></li>
<li><a href="/resources/storage_services.php">Storage Services</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/data_services.php">Data Transfer Services</a></li>
<li><a href="/resources/virtualmachines.php">Virtual Machines</a></li>
<li><a href="/resources/hosting.php">Hosting Services</a></li>
<li><a href="/resources/Networking.php">Networking</a></li>
<li><a href="/resources/ProtectedEnvironment.php">Protected Environment</a></li>
<li><a href="/documentation/software/ai.php">ML/AI Resources</a></li>
<li><a href="/userservices/index.php">User Services</a></li>
</ul>
</li>
<li class="has-sub sub-width-lg"><a href="#">Documentation</a>
<ul class="sub-menu">
<li><a href="https://www.chpc.utah.edu/documentation/index.php">Documentation</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/gettingstarted.php">Getting Started</a></li>
<li><a href="https://www.chpc.utah.edu/resources/access.php">Accessing Our Resources</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/videos/index.php">Short Training Videos</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/guides/index.php">Cluster Guides</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/guides/beehive.php">Beehive User Guide (Windows Server)</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/guides/gpus-accelerators.php">GPU &amp; Accelerators</a></li>
<li><a href="/documentation/software/ai.php">ML/AI Resources</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/guides/frisco-nodes.php">Frisco Nodes</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/guides/narwhal.php">Narwhal User Guide (Protected Environment Statistics)</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/software/index.php">Software</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/ProgrammingGuide.php">Programming Guide</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/policies/index.php">Policy Manual</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/data_services.php">Data Transfer Services</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/guides/HelloWorldMPI.php">HPC Basics - Hello World MPI</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/faq.php">Frequently Asked Questions</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/white_papers/index.php">White Papers</a></li>
</ul>
</li>
<li class="has-sub"><a href="#">User Services</a>
<ul class="sub-menu">
<li><a href="/userservices/index.php">User Services</a></li>
<li><a href="https://www.chpc.utah.edu/userservices/accounts.php">Accounts</a></li>
<li><a href="/userservices/allocations.php">Allocations</a></li>
<li><a href="/userservices/gettinghelp.php">Getting Help</a></li>
<li><a href="https://www.chpc.utah.edu/presentations/index.php">Training</a></li>
</ul>
</li>
<li class="has-sub sub-width-lg"><a href="#">Usage</a>
<ul class="sub-menu">
<li><a href="/usage/cluster/index.php">Cluster Usage</a>
<ul>
<li><a href="/usage/cluster/current-project-general.php">General Allocation Pool</a></li>
<li><a href="/usage/cluster/current-project-lonepeak.php">Lonepeak Cluster</a></li>
<li><a href="/usage/cluster/current-project-kingspeak.php">Kingspeak Cluster</a></li>
<li><a href="/usage/cluster/current-project-ash.php">Ash Cluster</a></li>
<li><a href="/usage/cluster/current-project-redwood.php">Redwood Cluster</a></li>
</ul>
</li>
<li><a href="/usage/graphs.php?g=cluster%20utilization&amp;host=combined&amp;type=daily_utilization">Cluster Utilization Graphs</a></li>
<li><a href="/usage/graphs.php?g=hpc%20cluster%20scratch&amp;host=chpc_gen&amp;type=cpu">HPC Cluster Scratch</a></li>
<li><a href="/usage/graphs.php?g=network&amp;host=Campus+Gateway&amp;type=daily_traffic">Network</a>
<ul>
<li><a href="/resources/maddash-dashboards.php">Maddash Dashboards</a></li>
</ul>
</li>
<li><a href="http://weathermap.uen.net/" target="_blank" rel="noopener">UEN Weathermap (Only Available on UEN Networks)</a></li>
<li><a href="http://snapp2.bldc.grnoc.iu.edu/i2al2s/#&amp;p=3%2C42&amp;ccid=3&amp;tab=1&amp;search=undefined&amp;pwidth=undefined&amp;ccat=undefined&amp;url=show-graph.cgi%3Fcollection_ids%3D145%26end%3D1461772131%26start%3D1461771831%26cf%3DAVERAGE%26ds%3Doutput%2Cinput%26collection_ids%3D145" target="_blank" rel="noopener">UEN Aggregate Utilization</a></li>
<li><a href="http://uofu.status.io">UofU IT Services Status</a></li>
<li><a href="https://status.it.utah.edu">University Application Heath Summary - NOC</a></li>
</ul>
</li>
<li><a href="/role/">My Account</a></li>
</ul>
</ul>	
</nav>

<!-- END HEADER NAVIGATION -->
                     </div>
                  <div class="uu-search-toggle"><button class="uu-search__trigger"><span class="far fa-search" aria-hidden="true"></span><span class="sr-only">Search</span></button></div><button id="jsNavTrigger" class="uu-nav__trigger" aria-haspopup="true" aria-expanded="false"><span class="sr-only">Reveal Menu</span><span></span></button></div>
            </div>
         </header>
         <!-- END HEADER -->
         <!-- PUSH NAVIGATION -->
         
         <section class="uu-nav">
            <div class="uu-nav__container"><button id="jsMobileNavTrigger" class="uu-nav__trigger" aria-haspopup="true" aria-expanded="false"><span class="sr-only">Reveal Menu</span><span></span></button><header class="uu-nav__header">
                  <h2 class="sr-only">Main Navigation</h2>
                  <!-- Navigation Logo -->
<a href="https://utah.edu/" class="uu-nav__logo">
	<img src="https://templates.utah.edu/_main-v3-1/images/template/university-of-utah-logo.svg" alt="The University of Utah"/>
</a></header>
               <nav class="uu-menu"><p><h2 class="uu-menu__title">Main Menu</h2>
<hr />
<ul class="uu-menu__level1">
<li><a href="/">Home</a></li>
<li class="has-sublist"><a href="#">About Us</a>
<ul class="uu-menu__level2">
<li><a href="/about/index.php">About Us</a></li>
<li><a href="https://www.chpc.utah.edu/about/vision.php">Vision</a></li>
<li><a href="/about/staff.php">Staff</a></li>
<li><a href="https://www.chpc.utah.edu/about/contact.php">Contact Information</a></li>
<li><a href="https://www.chpc.utah.edu/about/acknowledge.php">Acknowledging CHPC</a></li>
<!--<li><a href="/about/bibliography/CHPC%20BIB.pdf" target="_blank" rel="noopener">CHPC Bibliography</a></li>-->
<li><a href="https://www.chpc.utah.edu/about/SupportedResearch.php">Supported Research</a></li>
<li><a href="https://www.chpc.utah.edu/highlights.php">Research Highlights</a></li>
<li><a href="https://www.chpc.utah.edu/about/partners.php">Partners</a></li>
<li><a href="/news/index.php">News</a></li>
<li><a title="University Information Technology" href="https://it.utah.edu/" target="_blank" rel="noopener">UIT</a></li>
<li><a href="/about/governance.php">Governance</a></li>
</ul>
</li>
<li class="has-sublist"><a href="#">Resources</a>
<ul class="uu-menu__level2">
<li><a href="https://www.chpc.utah.edu/resources/index.php">Resources</a></li>
<li><a href="https://www.chpc.utah.edu/resources/HPC_Clusters.php">HPC Clusters</a></li>
<li><a href="/resources/storage_services.php">Storage Services</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/data_services.php">Data Transfer Services</a></li>
<li><a href="/resources/virtualmachines.php">Virtual Machines</a></li>
<li><a href="/resources/hosting.php">Hosting Services</a></li>
<li><a href="/resources/Networking.php">Networking</a></li>
<li><a href="/resources/ProtectedEnvironment.php">Protected Environment</a></li>
<li><a href="/documentation/software/ai.php">AI/ML Resources</a></li>
<li><a href="/userservices/index.php">User Services</a></li>
</ul>
</li>
<li class="has-sublist"><a href="#">Documentation</a>
<ul class="uu-menu__level2">
<li><a href="https://www.chpc.utah.edu/documentation/index.php">Documentation</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/gettingstarted.php">Getting Started</a></li>
<li><a href="https://www.chpc.utah.edu/resources/access.php">Accessing Our Resources</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/videos/index.php">Short Training Videos</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/guides/index.php">Cluster Guides</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/guides/beehive.php">Beehive User Guide (Windows Server)</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/guides/gpus-accelerators.php">GPU &amp; Accelerators</a></li>
<li><a href="/documentation/software/ai.php">ML/AI Resources</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/guides/frisco-nodes.php">Frisco Nodes</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/guides/narwhal.php">Narwhal User Guide (Protected Environment Statistics)</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/software/index.php">Software</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/ProgrammingGuide.php">Programming Guide</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/policies/index.php">Policy Manual</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/data_services.php">Data Transfer Services</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/guides/HelloWorldMPI.php">HPC Basics - Hello World MPI</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/faq.php">Frequently Asked Questions</a></li>
<li><a href="https://www.chpc.utah.edu/documentation/white_papers/index.php">White Papers</a></li>
</ul>
</li>
<li class="has-sublist"><a href="#">User Services</a>
<ul class="uu-menu__level2">
<li><a href="/userservices/index.php">User Services</a></li>
<li><a href="https://www.chpc.utah.edu/userservices/accounts.php">Accounts</a></li>
<li><a href="/userservices/allocations.php">Allocations</a></li>
<li><a href="/userservices/gettinghelp.php">Getting Help</a></li>
<li><a href="https://www.chpc.utah.edu/presentations/index.php">Training</a></li>
</ul>
</li>
<li class="has-sublist"><a href="#">Usage</a>
<ul class="uu-menu__level2">
<li class="has-sublist"><a href="/usage/cluster/index.php">Cluster Usage</a>
<ul class="uu-menu__level3">
<li><a href="/usage/cluster/current-project-general.php">General Allocation Pool</a></li>
<li><a href="/usage/cluster/current-project-lonepeak.php">Lonepeak Cluster</a></li>
<li><a href="/usage/cluster/current-project-kingspeak.php">Kingspeak Cluster</a></li>
<li><a href="/usage/cluster/current-project-ash.php">Ash Cluster</a></li>
<li><a href="/usage/cluster/current-project-redwood.php">Redwood Cluster</a></li>
</ul>
</li>
<li><a href="/usage/graphs.php?g=cluster%20utilization&amp;host=combined&amp;type=daily_utilization">Cluster Utilization Graphs</a></li>
<li><a href="/usage/graphs.php?g=hpc%20cluster%20scratch&amp;host=chpc_gen&amp;type=cpu">HPC Cluster Scratch</a></li>
<li class="has-sublist"><a href="/usage/graphs.php?g=network&amp;host=Campus+Gateway&amp;type=daily_traffic">Network</a>
<ul class="uu-menu__level3">
<li><a href="/resources/maddash-dashboards.php">Maddash Dashboards</a></li>
</ul>
</li>
<li class="has-sublist"><a href="http://weathermap.uen.net/" target="_blank" rel="noopener">UEN Weathermap</a>
<ul class="uu-menu__level3">
<li><a href="http://weathermap.uen.net/" target="_blank" rel="noopener">(Only Available on UEN Networks)</a></li>
</ul>
</li>
<li><a href="http://snapp2.bldc.grnoc.iu.edu/i2al2s/#&amp;p=3%2C42&amp;ccid=3&amp;tab=1&amp;search=undefined&amp;pwidth=undefined&amp;ccat=undefined&amp;url=show-graph.cgi%3Fcollection_ids%3D145%26end%3D1461772131%26start%3D1461771831%26cf%3DAVERAGE%26ds%3Doutput%2Cinput%26collection_ids%3D145" target="_blank" rel="noopener">UEN Aggregate Utilization</a></li>
<li><a href="http://uofu.status.io">UofU IT Services Status</a></li>
<li><a href="https://status.it.utah.edu">University Application Heath Summary - NOC</a></li>
</ul>
</li>
<li><a href="/role/">My Account</a></li>
</ul></p></nav>
            </div>
         </section>
         <!-- END PUSH NAVIGATION -->
         
         <!-- MAIN CONTENT -->
         <main class="uu-main" id="skip-link-target">
            <nav aria-label="Breadcrumb" class="uu-breadcrumb">
               <ol>
                  <li><a href="/"></a></li>
                  <li><a href="/documentation/">documentation</a></li>
                  <li><a href="/documentation/software/">software</a></li>
                  <li><span class="sr-only">Current Page: </span>Single executable on all CHPC platforms</li>
               </ol>
            </nav>
            <!-- SECTION 1 -->
            
            <section class="uu-section bg-white text-default uu-section--region-1" style="">
               <div class="uu-section__container"><!-- SECTION HEADER -->
                  
                  <div class="uu-section__header  ">
                     <h1>Single executable on all CHPC platforms</h1>
                     					Advances in compiler technology and MPI libraries are starting to allow building
                     single executables optimized for multiple CPU architectures and running over multiple
                     networks. The document below summarizes how to achieve this on CHPC machines. Please,
                     note that not all compilers and MPIs allow this, therefore we detail how to do this
                     for those that work and what problems to expect with those that don't work.
                     
                     				</div>
                  <!-- END SECTION HEADER -->
                  <!-- REGION 1 -->
                  
                  <div class="uu-section__region bg-white text-default no-border">
                     <ul>
                        <li><a href="#sum">Short summary</a></li>
                        <li><a href="#about">General remarks</a></li>
                        <li><a href="#submit">Multi-architecture CPU optimization</a><br><ul>
                              <li><a href="#intel_cpu">Intel compilers</a></li>
                              <li><a href="#pgi_cpu">NVHPC compilers</a></li>
                              <li><a href="#gnu_cpu">GNU compilers</a></li>
                           </ul>
                        </li>
                        <li><a href="#mpi">MPI multiple network interfaces support</a><br><ul>
                              <li><a href="#impi">Intel MPI</a></li>
                              <li><a href="#mpich">MPICH</a></li>
                              <li><a href="#ompi">OpenMPI</a></li>
                           </ul>
                        </li>
                        <li><a href="#march_mpi">Multi-architecture CPU optimization with multi-network MPI<br></a><ul>
                              <li><a href="#impi_cpu">Intel MPI</a></li>
                              <li><a href="#mvapich2_cpu">MVAPICH2</a></li>
                              <li><a href="#mpich_cpu">MPICH</a></li>
                              <li><a href="#ompi_cpu">OpenMPI</a></li>
                              <li><a href="#hpl_4n">LAMMPS benchmark results</a></li>
                           </ul>
                        </li>
                        <li><a href="#recomm">Recommendations for different scenarios </a>- skip to here if you don't want to read all the details<a href="#recomm"><br></a><ul>
                              <li><a href="#intel_rec">Optimized application running on all CHPC Linux machines</a></li>
                              <li><a href="#gnu_rec">Optimized application using GNU compilers</a></li>
                              <li><a href="#pgi_rec">Optimized application using <span>NVHPC</span> compilers</a></li>
                           </ul>
                        </li>
                     </ul>
                     <h2><a id="sum"></a> Short summary</h2>
                     <p>In this document we show how to build a single executable optimized for multiple CPU
                        architectures that runs in parallel over multiple network types. This should be beneficial
                        for both CHPC staff and for users who need to run their applications optimally on
                        all CHPC clusters.</p>
                     <p>Moreover, we evaluate the common Application Binary Interface (ABI) for MPI as implemented
                        in several MPI distributions and show how a single executable can be run using several
                        different MPI distributions without the need to recompile.</p>
                     <p>A result of this is a single parallel executable that can run optimally on all CHPC
                        clusters and on three out of the four MPI distributions that we support.</p>
                     <p>&nbsp;</p>
                     <h2><a id="about"></a> General remarks</h2>
                     <p style="text-align: left;">CPU architectures change from generation to generation, affecting the data/instruction
                        processing and adding/modifying CPU instructions. A common trend recently has been
                        improving vectorization capabilities of the CPUs. As of mid 2018, CHPC runs four generations
                        of Intel CPUs -- Nehalem, SandyBridge, Haswell and Skylake --&nbsp;each of which shows
                        incremental improvement in vectorization processing power which is significant enough
                        to be important be able to harnessed optimally.</p>
                     <p style="text-align: left;">Starting with AVX, Intel CPUs feature increasingly complex logic of clock speed adjustments
                        depending on how many CPU cores and vector units are being used. If less cores and
                        vectorization is utilized, the CPU may run at considerably larger clock speed than
                        when all cores/vector units are used. A good review of this beavior is at <a href="https://www.anandtech.com/show/11544/intel-skylake-ep-vs-amd-epyc-7000-cpu-battle-of-the-decade/8" target="_blank" rel="noopener">this Anandtech webpage</a>. For this reason the parallel performance of the current CPUs may not scale linearly
                        with increased core utilization, although in utilizing all the cores and the most
                        performing vector units (e.g. AVX512 in Skylake) should still be the most efficient.</p>
                     <p style="text-align: left;">The two commercial compilers that CHPC licenses, Intel and PGI, both support building
                        multiple optimized code paths for different CPU architectures into a single executable,
                        alleviating the need to build separate executables for each CPU type. The open source
                        GNU compiler does not currently support this option.</p>
                     <p style="text-align: left;">Parallel programs add another complexity factor in the form of network interface over
                        which the parallel program (usually using an MPI library) runs. Most of CHPC clusters
                        feature high performance InfiniBand network, but, Lonepeak cluster nodes as well as
                        user desktops, have only slower Ethernet network, which in the past required using
                        separate MPIs built for one network or another.</p>
                     <p style="text-align: left;">Most of current MPIs allow to include multiple network channels into a single MPI
                        build, which then allows to run executable built with such MPI on several different
                        networks.</p>
                     <p style="text-align: left;">As such, we are getting to a point when it is possible to make a single high performance
                        executable that runs optimally on many CPU architectures and over many networks.</p>
                     <p style="text-align: left;">That said, due to the simplicity of build and deployment, as well as good performance,
                        we recommend using the Intel compiler and Intel MPI as the first choice for building
                        user applications and we are planning to build most of the applications that we support
                        in this manner.</p>
                     <h2><a id="submit"></a> Multi-architecture CPU optimization</h2>
                     <h3 id="TangentUserGuide-ThecreationofabatchscriptontheTangentcluster"><a id="intel_cpu"></a> Intel compilers</h3>
                     <p>Intel calls this approach automatic CPU dispatch, and it is invoked with the <code style="background-color: lightgray;">-ax </code>compiler flag. The current highest CPU architecture at CHPC includes AVX512 vectorization
                        instructions and is achieved with<code style="background-color: lightgray;">-axCORE-AVX512</code>. This option instructs the compiler to produce two binary paths, one for the AVX512
                        compatible CPU, and other for generic x86 CPU. So, the code will run optimally using
                        AVX512 on an AVX512 CPU, but, run suboptimally using only SSE vectorization on any
                        other CPU (including those having AVX and higher SSE vectorization instructions).</p>
                     <p>In order to build executable that vectorizes optimally on all CHPC clusters, that
                        is on Nehalem, SandyBridge, Haswell and Skylake generations of Intel Xeon CPUs, we
                        need to add specifications for those particular architectures, i.e. <code style="background-color: lightgray;">-axCORE-AVX512,CORE-AVX2,AVX,SSE4.2</code>. To verify this is indeed the case, one can enable detailed compiler reporting via
                        flags <code style="background-color: lightgray;">-diag-enable=all -qopt-report</code>and then examine compile report <code style="background-color: lightgray;">*.optrpt</code> files, one for each source file. There we should see the following sections reporting
                        optimizations for the given architectures:</p>
                     <!-- PANEL -->
                     <div class="c-panel bg-light-gray text-default has-filter  " data-filter-terms="sample1 sample2" style=" ">
                        <pre>Begin optimization report for: main(int, char **) [skylake_avx512]<br>Begin optimization report for: main(int, char **) [core_4th_gen_avx]<br>Begin optimization report for: main(int, char **) [core_2nd_gen_avx]<br>Begin optimization report for: main(int, char **) [core_i7_sse4_2]<br>Begin optimization report for: main(int, char **) [generic]</pre>
                     </div>
                     <!-- END PANEL -->
                     
                     <p>In general, the&nbsp;<code style="background-color: lightgray;">*.optrpt</code> files are a good place to look at to examine how well the compiler vectorized the
                        program, although even better tool is the graphical <a href="https://www.chpc.utah.edu/documentation/software/intel-parallelXE.php">Intel Advisor</a>.</p>
                     <p>Note that using &nbsp;the single optimization option <code style="background-color: lightgray;">-fast</code> does not build multiple target executables; to produce highly optimized code add
                        the flags <code style="background-color: lightgray;">-O3 -ipo</code>, e.g.</p>
                     <!-- PANEL -->
                     <div class="c-panel bg-light-gray text-default has-filter  " data-filter-terms="sample1 sample2" style=" ">
                        <pre>icc -O3 -ipo -axCORE-AVX512,CORE-AVX2,AVX,SSE4.2 hello_ser.c -vec-report<br>hello_ser.c(8): (col. 5) remark: LOOP WAS VECTORIZED<br>hello_ser.c(4): (col. 1) remark: main has been targeted for automatic cpu dispatch</pre>
                     </div>
                     <!-- END PANEL -->
                     
                     <p>Please, also note that we have had trouble with building some codes with this complex
                        multi-target option. If a program compilation fails, remove the SSE4.2 option. This
                        will cause not to build optimized code for&nbsp;Lonepeak, but, the program will still run
                        there using the generic code path, and most likely not be significantly slower.</p>
                     <p>Below are results of series of HPL benchmark runs compiled with Intel 15.0.1 compiler,
                        MKL 11.2, Intel MPI 5.0.1 and the -ax option vs. the -x option which optimizes for
                        a specified CPU target only. We ran this benchmark on 4 or 5 different nodes, reporting
                        average with standard deviation - due to fact that there have been small variances
                        from run to run due to system noise. The values are in GFlops per node.</p>
                     <table class="confluenceTable">
                        <tbody>
                           <tr>
                              <th class="confluenceTh">&nbsp;</th>
                              <th class="confluenceTh">-axCORE-AVX2,AVX,SSE4.2</th>
                              <th class="confluenceTh">-xSSE4.2</th>
                              <th class="confluenceTh" colspan="1">-xAVX</th>
                              <th class="confluenceTh" colspan="1">-xCORE-AVX2</th>
                              <th class="confluenceTh" colspan="1">Speedup vs. Ember</th>
                              <th class="confluenceTh" colspan="1">Cores/node</th>
                              <th class="confluenceTh" colspan="1">Core increase</th>
                           </tr>
                           <tr>
                              <td class="confluenceTd">Ember Westmere</td>
                              <td class="confluenceTd">119.85+/-1.34</td>
                              <td class="confluenceTd">120.23+/-0.59</td>
                              <td class="confluenceTd" colspan="1">&nbsp;</td>
                              <td class="confluenceTd" colspan="1">&nbsp;</td>
                              <td class="confluenceTd" colspan="1">1.00</td>
                              <td class="confluenceTd" colspan="1">12</td>
                              <td class="confluenceTd" colspan="1">1.00</td>
                           </tr>
                           <tr>
                              <td class="confluenceTd" colspan="1">Kingspeak Sandybridge</td>
                              <td class="confluenceTd" colspan="1">311.63+/-3.56</td>
                              <td class="confluenceTd" colspan="1">&nbsp;</td>
                              <td class="confluenceTd" colspan="1">310.73+/-3.58</td>
                              <td class="confluenceTd" colspan="1">&nbsp;</td>
                              <td class="confluenceTd" colspan="1">2.60</td>
                              <td class="confluenceTd" colspan="1">16</td>
                              <td class="confluenceTd" colspan="1">1.33</td>
                           </tr>
                           <tr>
                              <td class="confluenceTd" colspan="1">Kingspeak Haswell</td>
                              <td class="confluenceTd" colspan="1">765.90+/-6.48</td>
                              <td class="confluenceTd" colspan="1">&nbsp;</td>
                              <td class="confluenceTd" colspan="1">&nbsp;</td>
                              <td class="confluenceTd" colspan="1">765.48+/-7.49</td>
                              <td class="confluenceTd" colspan="1">6.39</td>
                              <td class="confluenceTd" colspan="1">24</td>
                              <td class="confluenceTd" colspan="1">2.00</td>
                           </tr>
                        </tbody>
                     </table>
                     <p>&nbsp;</p>
                     <p>From the table above, it is evident that the automatic CPU dispatch (-ax) flag produced
                        CPU specific optimized code and single executable runs on all the three platforms
                        optimally. Since most of this benchmark runtime is spent in the LAPACK routines, part
                        of the MKL library, it does not necessarily show the power of compiler optimization
                        for each of the CPU architecture, nevertheless, the code runs optimally on all the
                        platforms without giving an illegal instruction error which would be the case when
                        we would optimize only for the latest CPU architecture and run on an earlier one.</p>
                     <p>It is also worth noticing the increase of GFlops performance among the three generation,
                        doubling it per core with the AVX instruction set (2 vs. 4 double precision wide vector
                        unit) and making it ~3x faster with the AVX2 (adding the fused multiply-add instruction,
                        i.e. doing vector multiplication and addition in a single instruction).</p>
                     <h3 id="TangentUserGuide-Interactivejobs"><a id="pgi_cpu"></a> &nbsp; NVHPC compilers</h3>
                     <p>PGI compilers call this the unified binary. It is achieved by bundling the different
                        CPU architecture names into the&nbsp;<code style="background-color: lightgray;">-tp</code> compiler flag. For the three CHPC compiler architectures, this corresponds to&nbsp;<code style="background-color: lightgray;">-tp=nehalem,sandybridge,haswell,skylake</code> .</p>
                     <p>Notice that using single optimization option <code style="background-color: lightgray;">-fastsse</code> works fine, e.g.</p>
                     <!-- PANEL -->
                     <div class="c-panel bg-light-gray text-default has-filter  " data-filter-terms="sample1 sample2" style=" ">
                        <pre>nvc -fastsse -tp=nehalem,sandybridge,haswell,skylake -Minfo=unified,vect hello_ser.c<br>main:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8, PGI Unified Binary version for -tp=skylake-64<br>&nbsp;&nbsp;&nbsp;&nbsp; 16, Generated 2 alternate versions of the loop<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Generated vector simd code for the loop<br>main:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8, PGI Unified Binary version for -tp=haswell-64<br>&nbsp;&nbsp;&nbsp;&nbsp; 16, Generated 2 alternate versions of the loop<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Generated vector simd code for the loop<br>main:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8, PGI Unified Binary version for -tp=sandybridge-64<br>&nbsp;&nbsp;&nbsp;&nbsp; 16, Generated 3 alternate versions of the loop<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Generated vector simd code for the loop<br>main:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8, PGI Unified Binary version for -tp=nehalem-64<br>&nbsp;&nbsp;&nbsp;&nbsp; 16, Generated 3 alternate versions of the loop<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Generated vector simd code for the loop</pre>
                     </div>
                     <!-- END PANEL -->
                     
                     <p>However, there are problems with difficulty in building MPI distributions with the
                        unified binary approach, which we'll detail below.</p>
                     <p>It is possible to work around all these issues, however, it makes PGI builds a little
                        more cumbersome.</p>
                     <h3><a id="gnu_cpu"></a> GNU compilers</h3>
                     <p>GNU compilers (as far as we know) don't allow for multiple code paths in an executable
                        so one has to build optimized code for each CPU architecture. Also note that the gcc
                        8.5.0 series which is shipped with Rocky Linux 8, does have optimization flags for
                        recent Intel CPUs that are on the Notchpeak cluster, but, it does not support optimizations
                        for the AMD CPUs that are on Notchpeak. The AMD CPUs will take advantage of the older
                        AVX2 instructions, equivalent to the CPUs on the Kingspeak cluster. For Kingspeak,
                        use <code style="background-color: lightgray;">-march=sandybridge</code>, for Notchpeak Intel nodes, use flag <code style="background-color: lightgray;">-march=skylake-avx512</code>. For Notchpeak AMD nodes, use flag <code style="background-color: lightgray;">-march=broadwell</code>.</p>
                     <p>Newer versions of the GNU compilers are installed as needed that include support for
                        newer CPUs. Version 10.2.0 version includes <code style="background-color: lightgray;">-march=znver2</code> flag which includes specific optimizations for the Notchpeak AMD CPUs, for example:</p>
                     <!-- PANEL -->
                     <div class="c-panel bg-light-gray text-default has-filter  " data-filter-terms="sample1 sample2" style=" ">
                        <pre>module load gcc/10.2.0<br>gcc -march=znver2 hello_ser.c</pre>
                     </div>
                     <!-- END PANEL -->
                     
                     <p>&nbsp;</p>
                     <h2><a id="mpi"></a> MPI multiple network interfaces support</h2>
                     <h3 id="Buildingapplicationsformultiplenetworktargets-IntelMPI"><a id="impi"></a> Intel MPI</h3>
                     <p>Intel MPI supports multiple networks. To use</p>
                     <p><code style="background-color: lightgray;">module load intel impi</code></p>
                     <p>The choice of network is made by the <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/mpi-library-2019-over-libfabric.html" target="_blank" rel="noopener">FI_PROVIDER</a> variable, with allowed variables being {<samp class="codeph">verbs, tcp</samp><samp class="codeph">}.&nbsp;</samp>While the best available fabric should be selected by default, we have seen cases
                        when the application would crash at the start when the fabric is not specified. If
                        this happens, the network can be explicitly specified by</p>
                     <p>For Infiniband -- &nbsp;<code style="background-color: lightgray;">mpirun -genv FI_PROVIDER=verbs -np 2 ./latbw_impi</code></p>
                     <p>For ethernet --&nbsp;<code style="background-color: lightgray;">mpirun -genv FI_PROVIDER=tcp -np 2&nbsp; ./latbw_impi</code></p>
                     <p>Here's a table of latency/bandwidth. Similar performance can be expected on other
                        clusters.</p>
                     <div class="uu-table-wrap">
                        <table class="confluenceTable tablesorter">
                           <thead>
                              <tr class="sortableHeader">
                                 <th class="confluenceTh sortableHeader" data-column="0">&nbsp;</th>
                                 <th class="confluenceTh sortableHeader" data-column="1">Ash Latency [us]</th>
                                 <th class="confluenceTh sortableHeader" data-column="2">Ash Bandwidth [MB/s]</th>
                                 <th class="confluenceTh sortableHeader" data-column="3">KP Latency [us]</th>
                                 <th class="confluenceTh sortableHeader" data-column="4">KP Bandwidth [MB/s]</th>
                                 <th class="confluenceTh sortableHeader" data-column="5">NP Latency [us]</th>
                                 <th class="confluenceTh sortableHeader" data-column="6">NP Bandwidth [MB/s]</th>
                              </tr>
                           </thead>
                           <tbody>
                              <tr>
                                 <td class="confluenceTd">verbs</td>
                                 <td class="confluenceTd">2.05</td>
                                 <td class="confluenceTd">6254</td>
                                 <td class="confluenceTd">1.68</td>
                                 <td class="confluenceTd">3376.022</td>
                                 <td class="confluenceTd">1.20</td>
                                 <td class="confluenceTd">6191.268</td>
                              </tr>
                              <tr>
                                 <td class="confluenceTd">
                                    <p>tcp</p>
                                 </td>
                                 <td class="confluenceTd">23.31</td>
                                 <td class="confluenceTd">2719</td>
                                 <td class="confluenceTd">14.60</td>
                                 <td class="confluenceTd">1748.446</td>
                                 <td class="confluenceTd">13.20</td>
                                 <td class="confluenceTd">2421.430</td>
                              </tr>
                           </tbody>
                        </table>
                     </div>
                     <p>Notice that the tcp ran over the InfiniBand using ipoib therefore we see fairly decent
                        latencies and bandwidths.</p>
                     <p>A different way of launching can be chosen by <code style="background-color: lightgray;">-bootstrap</code> option. There are many different choices for this option; of value at CHPC are &nbsp;<code style="background-color: lightgray;">slurm</code> (the default if nothing is specified), and <code style="background-color: lightgray;">ssh</code>. To use <code style="background-color: lightgray;">ssh</code> over multiple nodes, one has to prepare a host file (as we were used in the PBS world),
                        and feed it to the mpirun:<br><code style="background-color: lightgray;">mpirun -genv FI_PROVIDER=verbs -bootstrap ssh -machinefile nodefile -np 2 ./latbw_impi</code></p>
                     <p>Now, the fun part is, that, depending on how we name the node names in the host file,
                        the tcp will run over the given network. If we use hostnames such as kp001,..., it'll
                        run over the Ethernet, if we use kp001.ipoib,..., it'll run over the InfiniBand using
                        ipoib.</p>
                     <p>Due to the simplicity of build and deployment, and good CPU affinity support, we recommend
                        using Intel MPI in user applications. However, be aware that OpenMPI and MVAPICH2
                        have better latencies. Applications that send many small messages will likely perform
                        the best with OpenMPI.</p>
                     <p>Also note that thanks to the common Application Binary Interface (ABI) between Intel
                        MPI (&gt;= 5.0) , MPICH (&gt;= 3.1) and MVAPICH2 (&gt;= 2.0), one can build a dynamic executable
                        (default on our systems) with one MPI, and run with the other. See details about this
                        at <a href="https://software.intel.com/en-us/articles/using-intelr-mpi-library-50-with-mpich3-based-applications" target="_blank" rel="noopener">https://software.intel.com/en-us/articles/using-intelr-mpi-library-50-with-mpich3-based-applications</a>.</p>
                     <h3><a id="mpich"></a> MPICH</h3>
                     <p>MPICH's most commonly used <code style="background-color: lightgray;">nemesis</code> channel supports several subchannels (called netmods), including <code style="background-color: lightgray;">tcp</code> , <code style="background-color: lightgray;">ib</code> , <code style="background-color: lightgray;">mxm</code> and <code style="background-color: lightgray;">ofi</code>. The last three use InfiniBand, however <code style="background-color: lightgray;">ib</code> and <code style="background-color: lightgray;">mxm</code> are deprecated. We therefore built MPICH from version 3.2.1 with the relatively new&nbsp;<code style="background-color: lightgray;">ofi</code> OpenFabrics netmod. As such, it does not appear to be optimized since latencies and
                        bandwidths seem to be lower than that of MVAPICH2 or Intel MPI. We therefore recommend
                        to build with these if the program will be run mostly on the InfiniBand clusters.</p>
                     <p>Since the&nbsp;<code style="background-color: lightgray;">tcp</code>&nbsp;netmod is default, to run over InfiniBand, set environment variable<code style="background-color: lightgray;"> MPICH_NEMESIS_NETMOD=ofi</code>, e.g.<br><code style="background-color: lightgray;">mpirun -genv MPICH_NEMESIS_NETMOD ofi -np 2 ./a.out</code></p>
                     <p>For a single workstation (running MPI program with multiple processes on a single
                        node), the network is not accessed and therefore the program works even if IB is not
                        present on the system.&nbsp;<code></code></p>
                     <p>Also, note that unlike Intel MPI or MVAPCH2, MPICH does not set task affinity to the
                        CPUs (i.e., &nbsp;binds tasks to CPUs for better performance) by default. To enable the
                        task affinity, you need to use flag<code style="background-color: lightgray;">-bind-to</code> , e.g. to bind each MPI task to a core, <code style="background-color: lightgray;">-bind-to=core</code>.</p>
                     <p>It is possible, however, to run the MPICH built executable on the clusters with InfiniBand,
                        taking advantage of the common ABI with Intel MPI. Simply add the Intel MPI to your
                        environment, with<br><code style="background-color: lightgray;">module load impi</code><br>and then run with Intel MPI as<br><code style="background-color: lightgray;">mpirun -np 2 ./a.out<br></code></p>
                     <h3><a id="ompi"></a> OpenMPI</h3>
                     <p>OpenMPI has provided multi-network support for a while, and chooses the best available
                        network automatically.</p>
                     <p>To force use Ethernet, add the <code style="background-color: lightgray;">--mca btl tcp,self</code> flag to mpirun, the default is to use the InfiniBand.</p>
                     <!-- PANEL -->
                     <div class="c-panel bg-light-gray text-default has-filter  " data-filter-terms="sample1 sample2" style=" ">
                        <pre>mpirun --mca btl tcp,self -np $SLURM_NTASKS $EXE # uses Ethernet<br>mpirun -np $SLURM_NTASKS $EXE                     # uses InfiniBand</pre>
                     </div>
                     <!-- END PANEL -->
                     
                     <p>Here's a table of latency/bandwidth on ember and kingspeak. Similar performance can
                        be expected on other clusters.</p>
                     <div class="scroll-overflow">
                        <table class="uu-table table-no-border">
                           <tbody>
                              <tr>
                                 <th class="confluenceTh">&nbsp;</th>
                                 <th class="confluenceTh" colspan="1">KP Latency [us]</th>
                                 <th class="confluenceTh" colspan="1">KP Bandwidth [MB/s]</th>
                                 <th class="confluenceTh" colspan="1">NP Latency [us]</th>
                                 <th class="confluenceTh" colspan="1">NP Bandwidth [MB/s]</th>
                              </tr>
                              <tr>
                                 <td class="confluenceTd">ib</td>
                                 <td class="confluenceTd" colspan="1">1.486</td>
                                 <td class="confluenceTd" colspan="1">3348.124</td>
                                 <td class="confluenceTd" colspan="1">2.962</td>
                                 <td class="confluenceTd" colspan="1">6081.060</td>
                              </tr>
                              <tr>
                                 <td class="confluenceTd">
                                    <p>tcp</p>
                                 </td>
                                 <td class="confluenceTd" colspan="1">15.189</td>
                                 <td class="confluenceTd" colspan="1">229.600</td>
                                 <td class="confluenceTd" colspan="1">18.405</td>
                                 <td class="confluenceTd" colspan="1">230.317</td>
                              </tr>
                           </tbody>
                        </table>
                     </div>
                     <p>It looks like the TCP bandwidth is limited to 1 Mbit/sec, which indicates that OpenMPI
                        chose the Ethernet network to run over.</p>
                     <p>Since OpenMPI does not yet support the common MPI ABI, its executable can not be exchanged
                        with intel MPI, MVAPICH2, and MPICH.</p>
                     <h2><a id="march_mpi"></a> Multi-architecture CPU optimization with multi-network MPI</h2>
                     <h3><a id="impi_cpu"></a> Intel MPI</h3>
                     <p>Intel MPI multi-network support along with the automatic CPU dispatch inside of the
                        Intel compilers is quite straightforward and works well. During the compilation, all
                        one needs is to include the <code style="background-color: lightgray;">-axCORE-AVX512,CORE-AVX2,AVX,SSE4.2</code> flag, <code style="background-color: lightgray;">-O3 -ip</code> to also include good optimization, and, then, at runtime select the appropriate network
                        fabrics via the I_MPI_FABRICS environment variable.</p>
                     <p>Intel MPI is thus a good choice for a single executable that runs optimized over all
                        CHPC Linux machines. As such, most of the applications that we support are built that
                        way.</p>
                     <p>Note that there is also&nbsp;<code style="background-color: lightgray;">mpitune</code> utility that allows one to run multiple scenarios automatically and come up with
                        the best Intel MPI runtime parameters (all MPIs feature a number of internal switches
                        that adjust communication parameters based on message sizes, task counts, etc). Details
                        on the <code style="background-color: lightgray;">mpitune</code> utility are at&nbsp; <a href="https://software.intel.com/en-us/node/528811" target="_blank" rel="noopener">Intel's mpitune page</a>.</p>
                     <h3><a id="mvapich2_cpu"></a> MVAPICH2</h3>
                     <p>The default network channel of MVAPICH2 &nbsp;is <code style="background-color: lightgray;">mrail</code>, which only supports InfiniBand, but, it is supposed to provides better performance
                        than the <code style="background-color: lightgray;">nemesis</code> channel which was inherited from MPICH. However, in reality based on the LAMMPS benchmarks
                        below (outdated) MPICH with&nbsp;<code style="background-color: lightgray;">mxm</code> performs faster. MVAPICH2's strength may lie in specific communication and InfiniBand
                        optimizations which LAMMPS may not be utilizing.</p>
                     <p>MVAPICH2's good performance on InfiniBand makes it a good choice for a program that
                        will run on InfiniBand clusters. If using Intel or PGI compilers, you can build executable
                        that can run optimally on all three CPU architectures that our clusters have.</p>
                     <p>Note that you can still run MVAPICH2 executables on Ethernet only clusters or on single
                        desktops using Intel MPI or MPICH libraries thanks to common ABI. In case the program
                        complains about missing libpmi.so,<br><code style="background-color: lightgray;">setenv LD_LIBRARY_PATH "/uufs/chpc.utah.edu/sys/pkg/slurm/std/lib:$LD_LIBRARY_PATH"</code>for tcsh<br>or <code style="background-color: lightgray;">export LD_LIBRARY_PATH "/uufs/chpc.utah.edu/sys/pkg/slurm/std/lib:$LD_LIBRARY_PATH"</code> for bash,<br>and then run the executable as if using Intel MPI or MPICH.</p>
                     <h3><a id="mpich_cpu"></a> MPICH</h3>
                     <p>MPICH with multi-network (over MXM), multi-architecture works well only with Intel
                        compilers, GNU does not support multi-architecture, whereas PGI has problems with
                        running unified binary-built MPICH on Sandybridge and higher CPUs. Therefore we only
                        provide &nbsp;MPICH built with Intel compiler and the <code style="background-color: lightgray;">-axCORE-AVX512,CORE-AVX2,AVX,SSE4.2</code> flag, along with &nbsp;the PGI and GNU MPICH builds with the lowest common denominator
                        optimization, which is the Nehalem architecture. Since there should be minimum vectorization
                        potential inside the MPI, this should not affect the performance radically. You can
                        then build PGI unified binary applications on top of the Nehalem optimized MPI.</p>
                     <p>Furthermore, from MPICH 3.2.1, the MXM interface is not included in our build, replaced
                        with OpenFabrics (ofi), which does not appear to be as well performing as MXM used
                        to be.</p>
                     <h3><a id="ompi_cpu"></a> OpenMPI</h3>
                     <p>OpenMPI with multi-network, multi-architecture works well with Intel compilers. GNU
                        does not support multi-architecture. PGI has problems with running unified binary-built
                        OpenMPI on Sandybridge and higher CPUs, which is why we have built OpenMPI optimized
                        for Nehalem as in case of MPICH. Build PGI unified binary on top of the Nehalem optimized
                        MPI.</p>
                     <h3><a id="hpl_4n"></a> LAMMPS benchmark results</h3>
                     <p>Below are benchmarks of LAMMPS molecular dynamics code results for jobs run on four
                        Kingspeak Haswell and SandyBridge nodes using Intel compilers and MPI (-axCORE-AVX2)
                        to build the code and running using different Intel MPI runtime option, and also MPICH
                        and MVAPICH2. Total runtime and communication time in seconds are reported in the
                        table below, which means lower number is better.</p>
                     <p>The runs were performed as follows:</p>
                     <ul>
                        <li>IMPI default<br>
                           <!-- PANEL -->
                           <div class="c-panel bg-light-gray text-default has-filter  " data-filter-terms="sample1 sample2" style=" ">
                              <pre>module load intel impi<br>set EXE = /uufs/chpc.utah.edu/sys/installdir/lammps/9Dec14/src/lmp_intel2015<br>mpirun -np $SLURM_NTASKS $EXE &lt; in.spce</pre>
                           </div>
                           <!-- END PANEL -->
                           </li>
                        <li>IMPI shm:dapl<br>
                           <!-- PANEL -->
                           <div class="c-panel bg-light-gray text-default has-filter  " data-filter-terms="sample1 sample2" style=" ">
                              <pre>module load intel impi<br>set EXE = /uufs/chpc.utah.edu/sys/installdir/lammps/9Dec14/src/lmp_intel2015<br>mpirun -genv I_MPI_FABRICS shm:dapl -np $SLURM_NTASKS $EXE &lt; in.spce</pre>
                           </div>
                           <!-- END PANEL -->
                           </li>
                        <li>IMPI shm:ofa<br>
                           <!-- PANEL -->
                           <div class="c-panel bg-light-gray text-default has-filter  " data-filter-terms="sample1 sample2" style=" ">
                              <pre>module load intel impi<br>set EXE = /uufs/chpc.utah.edu/sys/installdir/lammps/9Dec14/src/lmp_intel2015<br>mpirun -genv I_MPI_FABRICS shm:ofa -np $SLURM_NTASKS $EXE &lt; in.spce</pre>
                           </div>
                           <!-- END PANEL -->
                           </li>
                        <li>IMPI srun<br>
                           <!-- PANEL -->
                           <div class="c-panel bg-light-gray text-default has-filter  " data-filter-terms="sample1 sample2" style=" ">
                              <pre>module load intel impi<br>setenv I_MPI_PMI_LIBRARY /uufs/kingspeak.peaks/sys/pkg/slurm/std/lib/libpmi.so<br>set EXE = /uufs/chpc.utah.edu/sys/installdir/lammps/9Dec14/src/lmp_intel2015<br>srun -n $SLURM_NTASKS $EXE &lt; in.spce</pre>
                           </div>
                           <!-- END PANEL -->
                           </li>
                        <li>MPICH default<br>
                           <!-- PANEL -->
                           <div class="c-panel bg-light-gray text-default has-filter  " data-filter-terms="sample1 sample2" style=" ">
                              <pre>module load intel mpich2<br>setenv LD_LIBRARY_PATH "/uufs/chpc.utah.edu/sys/installdir/mpich/3.1.4i/lib:$LD_LIBRARY_PATH"<br>setenv MPICH_NEMESIS_NETMOD mxm<br>set EXE = /uufs/chpc.utah.edu/sys/installdir/lammps/9Dec14/src/lmp_intel2015<br>mpirun -np $SLURM_NTASKS $EXE &lt; in.spce</pre>
                           </div>
                           <!-- END PANEL -->
                           </li>
                        <li>MPICH affinity<br>
                           <!-- PANEL -->
                           <div class="c-panel bg-light-gray text-default has-filter  " data-filter-terms="sample1 sample2" style=" ">
                              <pre>module load intel mpich2<br>setenv LD_LIBRARY_PATH "/uufs/chpc.utah.edu/sys/installdir/mpich/3.1.4i/lib:$LD_LIBRARY_PATH"<br>setenv MPICH_NEMESIS_NETMOD mxm<br>set EXE = /uufs/chpc.utah.edu/sys/installdir/lammps/9Dec14/src/lmp_intel2015<br>mpirun -bind-to core -np $SLURM_NTASKS $EXE &lt; in.spce</pre>
                           </div>
                           <!-- END PANEL -->
                           </li>
                        <li>MVAPICH2 default<br>
                           <!-- PANEL -->
                           <div class="c-panel bg-light-gray text-default has-filter  " data-filter-terms="sample1 sample2" style=" ">
                              <pre>module load intel mvapich2<br>set EXE = /uufs/chpc.utah.edu/sys/installdir/lammps/9Dec14/src/lmp_intel2015<br>srun -n $SLURM_NTASKS $EXE &lt; in.spce</pre>
                           </div>
                           <!-- END PANEL -->
                           </li>
                     </ul>
                     <p>&nbsp;</p>
                     <table border="0">
                        <colgroup></colgroup>
                        <colgroup span="5"></colgroup>
                        <tbody>
                           <tr>
                              <td align="LEFT" height="19">&nbsp;</td>
                              <td align="LEFT">LAMMPS LJ</td>
                              <td align="LEFT">&nbsp;</td>
                              <td align="LEFT">&nbsp;</td>
                              <td align="LEFT">&nbsp;</td>
                              <td align="LEFT">&nbsp;</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">&nbsp;</td>
                              <td align="LEFT">96 procs, Haswell</td>
                              <td align="LEFT">&nbsp;</td>
                              <td align="LEFT">64 procs, Sandybridge</td>
                              <td align="LEFT">&nbsp;</td>
                              <td align="LEFT">&nbsp;</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">&nbsp;</td>
                              <td align="CENTER">Total</td>
                              <td align="CENTER">Comm</td>
                              <td align="CENTER">Total</td>
                              <td align="CENTER">Comm</td>
                              <td align="LEFT">&nbsp;</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">IMPI default</td>
                              <td align="CENTER">41.02</td>
                              <td align="CENTER">10.46</td>
                              <td align="CENTER">73.61</td>
                              <td align="CENTER">23.40</td>
                              <td align="LEFT">core affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">IMPI shm:dapl</td>
                              <td align="CENTER">40.79</td>
                              <td align="CENTER">10.25</td>
                              <td align="CENTER">73.55</td>
                              <td align="CENTER">23.31</td>
                              <td align="LEFT">core affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">IMPI shm:ofa</td>
                              <td align="CENTER">42.79</td>
                              <td align="CENTER">13.70</td>
                              <td align="CENTER">70.61</td>
                              <td align="CENTER">22.94</td>
                              <td align="LEFT">core affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">IMPI srun</td>
                              <td align="CENTER">64.15</td>
                              <td align="CENTER">30.48</td>
                              <td align="CENTER">91.71</td>
                              <td align="CENTER">38.14</td>
                              <td align="LEFT">no affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">MPICH mxm</td>
                              <td align="CENTER">64.93</td>
                              <td align="CENTER">31.07</td>
                              <td align="CENTER">89.07</td>
                              <td align="CENTER">36.36</td>
                              <td align="LEFT">no affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">MPICH mxm bind core</td>
                              <td align="CENTER">45.66</td>
                              <td align="CENTER">14.63</td>
                              <td align="CENTER">74.96</td>
                              <td align="CENTER">24.59</td>
                              <td align="LEFT">core affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">MVAPICH2 default</td>
                              <td align="CENTER">42.61</td>
                              <td align="CENTER">12.30</td>
                              <td align="CENTER">74.67</td>
                              <td align="CENTER">25.19</td>
                              <td align="LEFT">core affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">&nbsp;</td>
                              <td align="LEFT">LAMMPS SPCE</td>
                              <td align="LEFT">&nbsp;</td>
                              <td align="LEFT">&nbsp;</td>
                              <td align="LEFT">&nbsp;</td>
                              <td align="LEFT">&nbsp;</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">&nbsp;</td>
                              <td align="LEFT">96 procs, Haswell</td>
                              <td align="LEFT">&nbsp;</td>
                              <td align="LEFT">64 procs, Sandybridge</td>
                              <td align="LEFT">&nbsp;</td>
                              <td align="LEFT">&nbsp;</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">&nbsp;</td>
                              <td align="CENTER">Total</td>
                              <td align="CENTER">Comm</td>
                              <td align="CENTER">Total</td>
                              <td align="CENTER">Comm</td>
                              <td align="LEFT">&nbsp;</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">IMPI default</td>
                              <td align="CENTER">60.39</td>
                              <td align="CENTER">2.61</td>
                              <td align="CENTER">76.91</td>
                              <td align="CENTER">3.71</td>
                              <td align="LEFT">core affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">IMPI shm:dapl</td>
                              <td align="CENTER">60.16</td>
                              <td align="CENTER">2.60</td>
                              <td align="CENTER">76.78</td>
                              <td align="CENTER">3.72</td>
                              <td align="LEFT">core affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">IMPI shm:ofa</td>
                              <td align="CENTER">60.68</td>
                              <td align="CENTER">2.77</td>
                              <td align="CENTER">74.60</td>
                              <td align="CENTER">3.41</td>
                              <td align="LEFT">core affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">IMPI srun</td>
                              <td align="CENTER">84.37</td>
                              <td align="CENTER">4.14</td>
                              <td align="CENTER">100.87</td>
                              <td align="CENTER">4.76</td>
                              <td align="LEFT">no affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">MPICH mxm</td>
                              <td align="CENTER">85.82</td>
                              <td align="CENTER">5.17</td>
                              <td align="CENTER">106.45</td>
                              <td align="CENTER">5.48</td>
                              <td align="LEFT">no affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">MPICH mxm bind core</td>
                              <td align="CENTER">60.77</td>
                              <td align="CENTER">3.69</td>
                              <td align="CENTER">75.94</td>
                              <td align="CENTER">4.18</td>
                              <td align="LEFT">core affinity</td>
                           </tr>
                           <tr>
                              <td align="LEFT" height="19">MVAPICH2 default</td>
                              <td align="CENTER">60.58</td>
                              <td align="CENTER">3.03</td>
                              <td align="CENTER">78.09</td>
                              <td align="CENTER">4.44</td>
                              <td align="LEFT">core affinity</td>
                           </tr>
                        </tbody>
                     </table>
                     <p>&nbsp;</p>
                     <p>&nbsp;There are several observations that can be made:</p>
                     <ul>
                        <li>Some MPI distributions set process affinity by default (Intel MPI, MVAPICH2, OpenMPI
                           while others do not (MPICH). &nbsp;Slurm currently is not set up for task affinity and
                           as such srun with Intel MPI does not perform well (this is to be retested once we
                           roll Slurm affinity to the clusters). MVAPICH2 overrides the Slurm affinity settings
                           and sets its own.</li>
                        <li>Using the process affinity is a good idea for performance reasons.</li>
                        <li>Intel MPI's ofa and dapl fabrics performance is comparable, but, in some cases one
                           is slightly better than the other, and vice versa.</li>
                        <li>MPICH mxm netmod is close to be competitive to Intel MPI, it may improve with the
                           MPICH 3.2 release (mxm contribution from Mellanox) and/or our Mellanox OFED stack
                           update in the future.</li>
                        <li>MVAPICH2 is not as fast as we have hoped, Intel MPI is faster for LAMMPS.</li>
                     </ul>
                     <p>&nbsp;</p>
                     <h2><a id="recomm"></a> Recommendations for different scenarios</h2>
                     <p>To summarize the material presented above, here are our recommendations for optimal
                        performance for several common scenarios.</p>
                     <h3><a id="intel_rec"></a> For an optimized application running on all CHPC Linux machines</h3>
                     <p>Use the Intel compilers with Intel MPI and the automatic CPU dispatch with <code style="background-color: lightgray;">-axCORE-AVX512,CORE-AVX2,AVX,SSE4.2</code> compiler flag. It is simple and it works. Test the dapl and ofa fabrics and pick
                        up one that performs better using the I_MPI_FABRICS option to mpirun.</p>
                     <p>Alternatively, all other MPI distributions (MVAPICH2, MPICH, OpenMPI) work with the
                        Intel compiler and the&nbsp;<code style="background-color: lightgray;">-axCORE-AVX2,AVX,SSE4.2</code> option as well. If you use MVAPICH2, you can use the same executable to run on the
                        Ethernet clusters and desktops using MPICH or Intel MPI thanks to the common ABI (see
                        the <a href="#impi_cpu">Intel MPI section</a>).</p>
                     <h3><a id="gnu_rec"></a> For an optimized application using GNU compilers</h3>
                     <p>Since GNU does not allow for multi-architecture optimization, you have to build separate
                        executables for the Kingspeak SandyBridge nodes (16, 20 cores) and Haswell nodes (24
                        cores). Use the -march flag to specify the appropriate given CPU architecture (-march=westmere
                        - Lonepeak, -march=sandybridge - Kingspeak 16 and 20 core nodes, -march=haswell -
                        Kingspeak 24 and 28 core nodes, Notchpeak AMD nodes, -march=skylake - Notchpeak Intel
                        nodes).</p>
                     <p>As for what MPI to use, on the clusters, use Open MPI or MVAPICH2 for single threaded
                        programs and Intel MPI for multi-threaded program - the latter provides better CPU
                        affinity control.</p>
                     <p>Be aware that OpenMPI lacks the ABI compatibility with the other MPI distributions
                        which makes it less flexible.&nbsp;</p>
                     <h3><a id="pgi_rec"></a> For an optimized application using NVHPC compilers</h3>
                     <p>The NVHPC unified binary works fairly well with applications, however, we did not
                        have much success building MPI libraries with it, so, the MPIs in the chpc.utah.edu
                        branch have been built to the lowest common denominator (Lonepeak). However, we don't
                        expect any performance impact on using these MPI builds.</p>
                     <p>To then build application that will run optimized on all CHPC InfiniBand clusters,
                        use MVAPICH2 with -tp=nehalem,sandybridge,haswell,skylake compiler flag. For flexible
                        binary that works both on InfiniBand and on Ethernet, use Intel MPI or MPICH.</p>
                     <p>OpenMPI would be another good choice.</p>
                  </div>
                  <!-- END REGION 1 -->
                  <!-- SECTION FOOTER -->
                  
                  <div class="uu-section__footer  ">
                     <p></p>
                  </div>
                  <!-- END SECTION FOOTER -->
                  </div>
            </section>
            <!-- END SECTION 1 -->
            <!-- SECTION 2 -->
            <!-- END SECTION 2 -->
            <!-- SECTION 3 -->
            <!-- END SECTION 3 -->
            <!-- SECTION 4 -->
            <!-- END SECTION 4 -->
            <!-- SECTION 5 -->
            <!-- END SECTION 5 -->
            </main>
         <!-- END MAIN CONTENT -->
         <!-- FOOTER -->
         
         <footer class="uu-footer"><div class="uu-footer__top">
   <div class="uu-footer__top-container">
      <div class="uu-footer__top-col1"><a href="https://www.utah.edu"><img src="https://templates.utah.edu/_main-v3-1/images/template/blocku.svg" alt="The University of Utah" class="uu-block-logo"></a><div class="department-name">
            <h2>The Center For High Performance Computing</h2>
         </div>
         <div class="department-address">
            <p>155 S 1452 E, RM. 405<br>SLC, UT 84112-0190<br>801.585.3791&nbsp;</p>
         </div>
      </div>
      <div class="uu-footer__top-col2">
         <h2 class="footer-heading">Stay in Touch</h2>
         <hr>
         <ul>
            <li><a href="https://map.utah.edu/index.html?code=inscc">Find Us</a></li>
            <li><a href="https://www.chpc.utah.edu/about/contact.php">Contact Us</a></li>
            <li><a href="mailto:helpdesk@chpc.utah.edu">Webmaster</a></li>
         </ul>
      </div>
      <div class="uu-footer__top-col5">
         <h2 class="footer-heading">Quick Links</h2>
         <hr>
         <ul>
            <li><a href="https://www.utah.edu/a-z/">A-Z Index</a></li>
            <li><a href="https://people.utah.edu/uWho/basic.hml">Campus Directory</a></li>
            <li><a href="https://www.map.utah.edu">Campus Map</a></li>
            <li><a href="https://map.utah.edu/?allshuttle=on">Shuttle Tracker </a></li>
            <li><a href="https://cis.utah.edu/">CIS</a></li>
            <li><a href="https://www.umail.utah.edu/">UMail</a></li>
            <li><a href="https://attheu.utah.edu/">@ The U</a></li>
         </ul>
      </div>
   </div>
</div><div class="uu-footer__bottom">
   <div class="uu-footer__bottom-container">
      <div class="uu-footer__bottom-col1"><a href="https://www.utah.edu/"><img src="https://templates.utah.edu/_main-v3-1/images/template/university-of-utah-logo.svg" alt="The University of Utah" class="uu-site-logo"></a></div>
      <div class="uu-footer__bottom-col2">
         <div class="legal">
            <p>© 2024 The University of Utah</p>
            <ul>
               <li><a href="https://www.utah.edu/indigenous-land-acknowledgment/index.php">Indigenous Land Acknowledgment</a></li>
               <li><a href="https://www.utah.edu/nondiscrimination/">Nondiscrimination &amp; Accessibility</a></li>
               <li><a href="https://www.utah.edu/disclaimer/">Disclaimer</a></li>
               <li><a href="https://www.utah.edu/privacy/">Privacy</a></li>
               <li><a href="https://www.utah.edu/credits-v3.php">Credits &amp; Attributions</a></li>
               <li><a href="https://attheu.utah.edu/media-contacts/">Media Contacts</a></li>
               <li><span id="directedit"></span></li>
            </ul>
         </div>
      </div>
      <div class="uu-footer__bottom-col3">
         <ul class="uu-social-list">
            <li><a href="https://twitter.com/uutah"><span class="fa-brands fa-x-twitter" aria-hidden="true"></span><span class="sr-only">X</span></a></li>
            <li><a href="https://www.facebook.com/universityofutah"><span class="fab fa-facebook" aria-hidden="true"></span><span class="sr-only">Facebook</span></a></li>
            <li><a href="https://www.instagram.com/universityofutah/"><span class="fab fa-instagram" aria-hidden="true"></span><span class="sr-only">Instagram</span></a></li>
            <li><a href="https://www.youtube.com/user/theuniversityofutah"><span class="fab fa-youtube" aria-hidden="true"></span><span class="sr-only">Youtube</span></a></li>
         </ul>
      </div>
   </div>
</div></footer>
         <!-- END FOOTER -->
         </div>
      <!-- FOOT CODE -->
      <script src="//templates.utah.edu/_main-v3-1/js/main.min.js"></script>
      
      
      
      
      
      
      <script src="//templates.utah.edu/_main-v3-1/js/directedit.js"></script><script><!--
window.onload = function(){ directedit(); }
//
			--></script>
      <script src="/_resources/js/custom.js"></script>
            
      <!-- END FOOT CODE -->
      
      <div id="hidden"><a id="de" href="https://a.cms.omniupdate.com/11/?skin=utah&amp;account=utah_home&amp;site=chpc2&amp;action=de&amp;path=/documentation/software/single-executable.pcf">Last Updated: 7/5/23</a></div>
      <!-- END PAGE BODY -->
      </body>
</html>